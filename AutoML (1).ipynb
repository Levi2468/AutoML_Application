{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ePCf8m5HS7s",
        "outputId": "84585a97-d42a-4cec-a487-07fccff840ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.2)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.12/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.28.9)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.6.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.12.12)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit lime pyngrok xgboost shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AutoML Application\",\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "st.title(\"ü§ñ AutoML Application\")\n",
        "st.subheader(\"Build, Train, Predict & Explain ML Models Automatically\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "---\n",
        "\n",
        "### üì§ **Step 1: Upload Dataset**\n",
        "Go to the **Drop** tab and upload your dataset (CSV / Excel).\n",
        "\n",
        "The application will:\n",
        "- Preview your dataset\n",
        "- Let you select the target (prediction) column\n",
        "- Allow dropping unnecessary columns (ID, serial number, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### üßπ **Step 2: Automatic Preprocessing**\n",
        "Once you confirm your selections, the app will automatically:\n",
        "- Detect **classification or regression**\n",
        "- Handle missing values\n",
        "- Encode categorical features\n",
        "- Remove duplicates and extreme outliers\n",
        "- Detect **class imbalance**\n",
        "- Apply **SMOTE automatically (only for training data)** when required\n",
        "\n",
        "> ‚ö†Ô∏è SMOTE is applied safely to avoid data leakage.\n",
        "\n",
        "---\n",
        "\n",
        "### üèãÔ∏è **Step 3: Intelligent Model Training**\n",
        "The system trains **multiple ML models automatically**, including:\n",
        "- Logistic / Linear Regression\n",
        "- Support Vector Machines (SVM)\n",
        "- KNN\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost\n",
        "\n",
        "Each model is evaluated using:\n",
        "- **ROC-AUC** (Classification)\n",
        "- **R¬≤ Score** (Regression)\n",
        "\n",
        "‚úÖ The best baseline model is selected\n",
        "‚úÖ Hyperparameter tuning is applied **only if it improves performance**\n",
        "‚úÖ The final best model is stored automatically\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Step 4: Prediction**\n",
        "After training:\n",
        "- Input fields are generated dynamically from your dataset\n",
        "- Supports both numerical & categorical features\n",
        "- One-click prediction\n",
        "- Shows:\n",
        "  - Predicted class / value\n",
        "  - Prediction probability (for classification)\n",
        "\n",
        "You can also:\n",
        "- Save predictions\n",
        "- Download prediction history as CSV\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Step 5: Model Explainability (SHAP / LIME)**\n",
        "Understand **why** the model made a prediction:\n",
        "- SHAP explanations for supported models\n",
        "- Automatic fallback when SHAP/LIME is not supported\n",
        "- Feature impact visualization\n",
        "- Safe handling for models without probability output\n",
        "\n",
        "> If a model cannot be explained, the app clearly informs the user.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Purpose of This Application**\n",
        "This AutoML platform is designed to:\n",
        "- Enable **non-technical users** to build ML models\n",
        "- Automate the **entire ML pipeline**\n",
        "- Prevent common ML mistakes (data leakage, imbalance bias)\n",
        "- Provide **transparent & explainable predictions**\n",
        "- Work with **any structured dataset**\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ How to Start?\n",
        "‚û°Ô∏è Upload a dataset from the **Drop** page to begin.\n",
        "\"\"\")\n",
        "\n",
        "st.success(\"‚úî Ready to build intelligent ML models without writing code!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF2xruGCoa8w",
        "outputId": "8a3c8414-07aa-403e-949a-fe87c2190990"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pages"
      ],
      "metadata": {
        "id": "2tdT1OzzPjlV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/1_Upload.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import kagglehub\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "st.set_page_config(page_title=\"AutoML\",page_icon='üëæ',layout='centered')\n",
        "\n",
        "if  \"df\"  in st.session_state:\n",
        "  st.title(\"üì• Dataset upload\")\n",
        "  st.success(\"‚úîÔ∏è File already uploaded\")\n",
        "  st.write(f\"**Preview: {st.session_state['dataset']}**\")\n",
        "  st.dataframe(st.session_state['df'].head())\n",
        "else:\n",
        "  def dataframe(Ufiles):\n",
        "    try:\n",
        "      dframe = pd.read_csv(Ufiles)\n",
        "    except Exception:\n",
        "      try:\n",
        "        dframe = pd.read_excel(Ufiles)\n",
        "      except Exception as e:\n",
        "        st.error(\"‚ùå Could not read file. Unsupported format\")\n",
        "        st.stop()\n",
        "    return dframe\n",
        "\n",
        "  def Multi_dataframe(path,Ufiles):\n",
        "      all_df=[]\n",
        "      for file_name in Ufiles:\n",
        "          st.write(file_name)\n",
        "          if file_name.endswith((\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\")):\n",
        "            all_df.append(dataframe(os.path.join(path,file_name)))\n",
        "      df_common = set(all_df[0].columns)\n",
        "      for dfs in all_df[1:]:\n",
        "        df_common = df_common.intersection(dfs.columns)\n",
        "      df_common = list(df_common)\n",
        "      st.write(f\"**üé∞Common features: {df_common[0]}**\")\n",
        "      df_merged = reduce(lambda df1, df2: pd.merge(df1, df2, on=df_common, how='outer'), all_df)\n",
        "      return df_merged\n",
        "\n",
        "  st.title(\"üì• Upload Dataset or Enter URL\")\n",
        "  uploaded_file = st.file_uploader(\"Upload CSV/Excel file or ZIP  file\", type=[\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\",\"zip\"])\n",
        "  st.write(\"***\")\n",
        "  data_url = st.text_input(\"Enter URL :\",placeholder=\"Paste a direct CSV/Excel or Kaggle Hub link here\")\n",
        "  df=None\n",
        "\n",
        "  if uploaded_file is not None:\n",
        "    if uploaded_file.name.split('.')[-1] ==\"zip\":\n",
        "      path=\"Data_folder\"\n",
        "      st.write(\"Multiple datasets detected...\")\n",
        "      with zipfile.ZipFile(uploaded_file, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(path)\n",
        "        files = os.listdir(path)\n",
        "        with st.spinner(\"Merging all Dataset\"):\n",
        "          df=Multi_dataframe(path,files)\n",
        "        st.session_state['df']=df\n",
        "        st.session_state[\"dataset\"]=uploaded_file.name.split('.')[0]\n",
        "    else:\n",
        "      df=dataframe(uploaded_file)\n",
        "      st.success(\"File uploaded successfully!\")\n",
        "      st.session_state['df']=df\n",
        "      st.session_state[\"dataset\"]=uploaded_file.name.split('.')[0]\n",
        "  elif data_url:\n",
        "    if data_url.split('.')[-1] not in (\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\"):\n",
        "      st.write(\"**Kaggle hub dataset**\")\n",
        "      path = kagglehub.dataset_download(data_url)\n",
        "      files = os.listdir(path)\n",
        "      if len(files)>1:\n",
        "        st.write(\"**üìöMultiple datasets detected**\")\n",
        "        with st.spinner(\"**üìíMerging all Dataset**\"):\n",
        "          df=Multi_dataframe(path,files)\n",
        "      else:\n",
        "        df = dataframe(os.path.join(path, files[0]))\n",
        "    else:\n",
        "      df=dataframe(data_url)\n",
        "    st.session_state[\"dataset\"]=data_url.split('/')[-1]\n",
        "    st.session_state['df']=df\n",
        "  else:\n",
        "    pass\n",
        "  if df is not None:\n",
        "    st.write(f\"**Preview: {st.session_state['dataset']}**\")\n",
        "    st.dataframe(st.session_state['df'].head())\n"
      ],
      "metadata": {
        "id": "ACi49vV4jZQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6708d1cc-d177-4d68-ee29-0a918581576a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/1_Upload.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/2_preprocessing.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "\n",
        "st.set_page_config(page_title=\"Drop_Test_Predict\", page_icon='üëæ', layout='centered')\n",
        "st.title(\"ü§ñ Train Model\")\n",
        "\n",
        "# ---------------- CHECK DATA ----------------\n",
        "if \"df\" not in st.session_state:\n",
        "    st.error(\"‚ùå No dataset uploaded. Go to Drop page first.\")\n",
        "    st.stop()\n",
        "\n",
        "# If already preprocessed\n",
        "if \"X\" in st.session_state and \"Y\" in st.session_state:\n",
        "    st.success(\"‚ú® Preprocessing Complete! Data is ready for training üöÄ\")\n",
        "    st.write(\"üßÆ Features Preview\")\n",
        "    st.dataframe(st.session_state[\"X\"].head())\n",
        "    st.write(\"üéØ Target Preview\")\n",
        "    st.dataframe(st.session_state[\"Y\"].head())\n",
        "    st.stop()\n",
        "\n",
        "df = st.session_state[\"df\"]\n",
        "st.dataframe(df.head())\n",
        "\n",
        "# ---------------- USER INPUT ----------------\n",
        "target_col = st.selectbox(\n",
        "    \"üéØ Select Target Column (Prediction Output Column)\",\n",
        "    options=df.columns\n",
        ")\n",
        "\n",
        "drop_cols = st.multiselect(\n",
        "    \"üóë Select Columns to Drop (ID / Serial / Irrelevant)\",\n",
        "    options=[c for c in df.columns if c != target_col]\n",
        ")\n",
        "\n",
        "# ---------------- HELPER ----------------\n",
        "def check_imbalance(y, threshold=0.4):\n",
        "    counts = Counter(y.squeeze())\n",
        "    ratio = min(counts.values()) / max(counts.values())\n",
        "    return ratio < threshold, ratio, dict(counts)\n",
        "\n",
        "# ---------------- APPLY ----------------\n",
        "if st.button(\"Apply Selection\"):\n",
        "\n",
        "    df = df.drop(columns=drop_cols)\n",
        "\n",
        "    target = df[target_col]\n",
        "\n",
        "    # Detect problem type\n",
        "    if target.dtype in [\"int64\", \"float64\"]:\n",
        "        p_type = \"classification\" if target.nunique() <= 20 else \"regression\"\n",
        "    else:\n",
        "        p_type = \"classification\"\n",
        "\n",
        "    st.session_state[\"p_type\"] = p_type\n",
        "    st.write(f\"Detected problem type: **{p_type.upper()}**\")\n",
        "\n",
        "    with st.spinner(\"üöÄ Preprocessing dataset...\"):\n",
        "\n",
        "        # Remove duplicates & missing target\n",
        "        df = df.drop_duplicates()\n",
        "        df = df.dropna(subset=[target_col])\n",
        "\n",
        "        X = df.drop(columns=target_col)\n",
        "        Y = df[target_col]\n",
        "\n",
        "        # Store categorical options\n",
        "        cat_options = {}\n",
        "        for col in X.select_dtypes(include=\"object\"):\n",
        "            cat_options[col] = X[col].unique().tolist()\n",
        "\n",
        "        st.session_state[\"cat_options\"] = cat_options\n",
        "\n",
        "        # Handle missing values & encoding\n",
        "        encoders = {}\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().mean() > 0.5:\n",
        "                X.drop(columns=col, inplace=True)\n",
        "                continue\n",
        "\n",
        "            if X[col].dtype in [\"int64\", \"float64\"]:\n",
        "                X[col].fillna(X[col].median(), inplace=True)\n",
        "                Q1 = X[col].quantile(0.25)\n",
        "                Q3 = X[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                no_of_outliers=((X[col]<lower_bound)|(X[col]>upper_bound)).sum()\n",
        "                if (no_of_outliers/len(X[col]))>0.1:\n",
        "                  X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "            else:\n",
        "                X[col].fillna(X[col].mode()[0], inplace=True)\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                encoders[col] = le\n",
        "\n",
        "        st.session_state[\"encoders\"] = encoders\n",
        "\n",
        "        # Target handling\n",
        "        if p_type == \"classification\":\n",
        "            target_encoder = LabelEncoder()\n",
        "            Y = pd.DataFrame(\n",
        "                target_encoder.fit_transform(Y.astype(str)),\n",
        "                columns=[target_col]\n",
        "            )\n",
        "            st.session_state[\"target_encoder\"] = target_encoder\n",
        "\n",
        "            # ---- IMBALANCE CHECK ----\n",
        "            is_imbalanced, ratio, dist = check_imbalance(Y)\n",
        "            st.session_state[\"is_imbalanced\"] = is_imbalanced\n",
        "            st.session_state[\"imbalance_ratio\"] = ratio\n",
        "            st.session_state[\"class_distribution\"] = dist\n",
        "\n",
        "            if is_imbalanced:\n",
        "                st.warning(\n",
        "                    f\"‚ö† Dataset is imbalanced (ratio={ratio:.2f}). \"\n",
        "                    \"SMOTE will be applied during training.\"\n",
        "                )\n",
        "            else:\n",
        "                st.success(\"‚úî Dataset is balanced\")\n",
        "\n",
        "        else:\n",
        "            #Regression outlier handling\n",
        "            Q1, Q3 = Y.quantile(0.25), Y.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            Y = Y.clip(Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
        "\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        st.session_state[\"X\"] = X\n",
        "        st.session_state[\"Y\"] = Y\n",
        "\n",
        "        st.success(\"‚ú® Preprocessing Complete!\")\n",
        "        st.dataframe(X.head())\n",
        "        st.dataframe(Y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2yWeBPtsM_J",
        "outputId": "8c2c0d7a-74c8-411d-9893-d597465ab7cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/2_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/3_train.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ---------------- STREAMLIT ----------------\n",
        "st.set_page_config(page_title=\"Train\", page_icon=\"üèãÔ∏è\", layout=\"centered\")\n",
        "st.title(\"üîÑ Model Training\")\n",
        "\n",
        "if \"X\" not in st.session_state or \"Y\" not in st.session_state:\n",
        "    st.error(\"‚ùå Please preprocess the dataset first.\")\n",
        "    st.stop()\n",
        "\n",
        "if \"best_model\" in st.session_state:\n",
        "    st.success(\"üèÜ Model already trained\")\n",
        "    st.write(f\"**{st.session_state['best_tuned_model_name']}**\")\n",
        "    st.write(f\"Score: {st.session_state['best_tuned_score']:.4f}\")\n",
        "    st.stop()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "X = st.session_state[\"X\"]\n",
        "Y = st.session_state[\"Y\"]\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "\n",
        "# Stratify if classification\n",
        "stratify = Y if p_type == \"classification\" else None\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=stratify\n",
        ")\n",
        "\n",
        "# ---------------- SMOTE ----------------\n",
        "if p_type == \"classification\" and st.session_state.get(\"is_imbalanced\", False):\n",
        "    min_class = min(st.session_state[\"class_distribution\"].values())\n",
        "    if min_class >= 10:\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "        st.info(\"‚öñ SMOTE applied on training data\")\n",
        "    else:\n",
        "        st.warning(\"‚ö† Too few minority samples ‚Äî SMOTE skipped\")\n",
        "\n",
        "# ---------------- MODELS ----------------\n",
        "if p_type == \"classification\":\n",
        "    models = {\n",
        "        \"Logistic Regression\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", LogisticRegression(max_iter=1000))\n",
        "        ]),\n",
        "        \"SVM\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", SVC(probability=True))\n",
        "        ]),\n",
        "        \"KNN\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", KNeighborsClassifier())\n",
        "        ]),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42, class_weight=\"balanced\"),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"),\n",
        "        \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "    }\n",
        "else:\n",
        "    models = {\n",
        "        \"Linear Regression\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", LinearRegression())\n",
        "        ]),\n",
        "        \"SVR\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", SVR())\n",
        "        ]),\n",
        "        \"KNN Regressor\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", KNeighborsRegressor())\n",
        "        ]),\n",
        "        \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
        "        \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
        "        \"XGBoost Regressor\": XGBRegressor(random_state=42)\n",
        "    }\n",
        "\n",
        "# ---------------- BASELINE ----------------\n",
        "results = {}\n",
        "\n",
        "with st.spinner(\"üîç Training baseline models...\"):\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            if len(np.unique(y_test)) == 2:\n",
        "                score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "            else:\n",
        "                score = roc_auc_score(\n",
        "                    y_test, model.predict_proba(X_test), multi_class=\"ovr\"\n",
        "                )\n",
        "        else:\n",
        "            score = r2_score(y_test, y_pred)\n",
        "\n",
        "        results[name] = score\n",
        "\n",
        "results_df = (\n",
        "    pd.DataFrame(results.items(), columns=[\"Model\", \"Score\"])\n",
        "    .sort_values(\"Score\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "st.success(\"‚úÖ Baseline training complete\")\n",
        "st.dataframe(results_df.head(3))\n",
        "\n",
        "# ---------------- SELECT BEST ----------------\n",
        "best_model_name = results_df.iloc[0][\"Model\"]\n",
        "best_score = results_df.iloc[0][\"Score\"]\n",
        "\n",
        "best_model = clone(models[best_model_name])\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------- SAVE ----------------\n",
        "st.success(\"üèÜ Final Best Model Selected\")\n",
        "st.write(f\"**{best_model_name}**\")\n",
        "st.write(f\"Score: {best_score:.4f}\")\n",
        "\n",
        "st.session_state[\"best_model\"] = best_model\n",
        "st.session_state[\"best_tuned_model_name\"] = best_model_name\n",
        "st.session_state[\"best_tuned_score\"] = best_score\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apq7t82IJQRF",
        "outputId": "fd5a58f5-d559-4c29-ffc0-1f8775ecf99b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/3_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/4_predict.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "st.set_page_config(page_title=\"Predict\", page_icon=\"üéØ\", layout=\"centered\")\n",
        "st.title(\"üîÆ Make Prediction\")\n",
        "\n",
        "# Check if model exists\n",
        "if \"best_model\" not in st.session_state or \"X\" not in st.session_state:\n",
        "    st.error(\"‚ùå Model not trained yet. Please go to Train page first.\")\n",
        "    st.stop()\n",
        "\n",
        "model = st.session_state[\"best_model\"]\n",
        "X_train_cols = st.session_state[\"X\"].columns   # processed columns\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "encoders = st.session_state.get(\"encoders\", {})\n",
        "target_encoder = st.session_state.get(\"target_encoder\", None)\n",
        "cat_values = st.session_state.get(\"cat_options\", {})\n",
        "\n",
        "st.write(\"### üìù Enter values for prediction:\")\n",
        "\n",
        "# Create input controls dynamically\n",
        "input_values = {}\n",
        "for col in X_train_cols:\n",
        "    if col in encoders:  # categorical\n",
        "        options = cat_values[col] if col in cat_values else list(encoders[col].classes_)\n",
        "        input_values[col] = st.selectbox(f\"{col}\", options=options)\n",
        "    else:\n",
        "        input_values[col] = st.number_input(f\"{col}\", value=float(st.session_state[\"X\"][col].median()))\n",
        "\n",
        "# Convert input to dataframe\n",
        "input_df = pd.DataFrame([input_values])\n",
        "st.session_state['input_df']=input_df\n",
        "# Encode categorical values\n",
        "for col in input_df.columns:\n",
        "    if col in encoders:\n",
        "        try:\n",
        "            input_df[col] = encoders[col].transform(input_df[col].astype(str))\n",
        "        except:\n",
        "            st.warning(f\"‚ö† Unknown category in '{col}'. Using default value.\")\n",
        "            input_df[col] = encoders[col].transform([encoders[col].classes_[0]])\n",
        "\n",
        "# Prediction\n",
        "if st.button(\"Predict\"):\n",
        "  try:\n",
        "    prediction = model.predict(input_df)[0]\n",
        "    st.session_state['prediction'] = prediction  # store prediction\n",
        "    st.session_state['last_input'] = input_df\n",
        "    if p_type == \"classification\":\n",
        "        proba = model.predict_proba(input_df)[0]\n",
        "        st.session_state['proba']=proba\n",
        "\n",
        "        if target_encoder:\n",
        "            prediction_label = target_encoder.inverse_transform([int(prediction)])[0]\n",
        "        else:\n",
        "            prediction_label = prediction\n",
        "\n",
        "        st.success(f\"üéØ Predicted Class: **{prediction_label}**\")\n",
        "    else:  # Regression\n",
        "        st.success(f\"üìå Predicted Value: **{prediction:.4f}**\")\n",
        "\n",
        "  except Exception as e:\n",
        "        st.error(f\"‚ùå Prediction failed: {e}\")\n",
        "# ---- PRINT BUTTON ----\n",
        "if st.button(\"Print / Save Result\"):\n",
        "    if \"prediction\" not in st.session_state:\n",
        "        st.error(\"‚ö† Please make a prediction first.\")\n",
        "    else:\n",
        "        save_row = st.session_state['last_input'].copy()\n",
        "        save_row[\"prediction\"] = st.session_state['prediction']\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            save_row[\"probability\"] = float(st.session_state[\"proba\"].max())\n",
        "\n",
        "        save_file = \"prediction_history.csv\"\n",
        "\n",
        "        # Write/append to file\n",
        "        if not os.path.exists(save_file):\n",
        "            save_row.to_csv(save_file, mode='w', header=True, index=False)\n",
        "        else:\n",
        "            save_row.to_csv(save_file, mode='a', header=False, index=False)\n",
        "\n",
        "        st.success(\"üìÅ Prediction saved successfully!\")\n",
        "\n",
        "        # Add download button\n",
        "        df_saved = pd.read_csv(save_file)\n",
        "        st.download_button(\n",
        "            label=\"üì• Download Prediction History\",\n",
        "            data=df_saved.to_csv(index=False),\n",
        "            file_name=\"prediction_history.csv\",\n",
        "            mime=\"text/csv\"\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq3E_Kyq4sh",
        "outputId": "417e90ec-f52f-456a-f160-ed9eafa363ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/4_predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/5_Shap.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "st.set_page_config(page_title=\"Explainability\", page_icon=\"üîç\", layout=\"centered\")\n",
        "st.title(\"‚öñÔ∏è Model Explainability (SHAP / LIME)\")\n",
        "\n",
        "# ------------------ VALIDATION ------------------\n",
        "if \"input_df\" not in st.session_state or \"best_model\" not in st.session_state:\n",
        "    st.error(\"‚ùå Make a prediction first to view explainability.\")\n",
        "    st.stop()\n",
        "\n",
        "model = st.session_state[\"best_model\"]\n",
        "input_df = st.session_state[\"input_df\"]\n",
        "X_train = st.session_state[\"X\"]\n",
        "Y_train = st.session_state[\"Y\"]\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "prediction = st.session_state[\"prediction\"]\n",
        "target_encoder = st.session_state.get(\"target_encoder\", None)\n",
        "\n",
        "explained = False   # track whether explanation succeeded\n",
        "\n",
        "# ======================================================\n",
        "# üîπ TRY SHAP\n",
        "# ======================================================\n",
        "try:\n",
        "    st.info(\"ü™µ Trying SHAP explanation...\")\n",
        "\n",
        "    shap.initjs()\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(input_df)\n",
        "\n",
        "    if p_type == \"classification\":\n",
        "        pred_idx = int(prediction)\n",
        "\n",
        "        if isinstance(shap_values, list):  # multiclass\n",
        "            values = shap_values[pred_idx][0]\n",
        "            base = explainer.expected_value[pred_idx]\n",
        "        else:  # binary\n",
        "            values = shap_values[0]\n",
        "            base = explainer.expected_value\n",
        "    else:\n",
        "        values = shap_values[0]\n",
        "        base = explainer.expected_value\n",
        "\n",
        "    explanation = shap.Explanation(\n",
        "        values=values,\n",
        "        base_values=base,\n",
        "        data=input_df.iloc[0].values,\n",
        "        feature_names=input_df.columns.tolist()\n",
        "    )\n",
        "\n",
        "    st.success(\"‚úî SHAP explanation generated\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    shap.plots.waterfall(explanation, show=False)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    explained = True\n",
        "\n",
        "except Exception:\n",
        "    st.warning(\"‚ö† SHAP not supported for this model.\")\n",
        "\n",
        "# ======================================================\n",
        "# üîπ TRY LIME (ONLY IF SHAP FAILED)\n",
        "# ======================================================\n",
        "if not explained:\n",
        "    try:\n",
        "        st.info(\"üçã Trying LIME explanation...\")\n",
        "\n",
        "        if p_type == \"classification\" and not hasattr(model, \"predict_proba\"):\n",
        "            raise NotImplementedError(\"Model has no probability scores\")\n",
        "\n",
        "        lime_explainer = LimeTabularExplainer(\n",
        "            training_data=X_train.values,\n",
        "            feature_names=X_train.columns.tolist(),\n",
        "            class_names=np.unique(Y_train).astype(str),\n",
        "            discretize_continuous=True\n",
        "        )\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            pred_idx = int(prediction)\n",
        "\n",
        "            label_name = (\n",
        "                target_encoder.inverse_transform([pred_idx])[0]\n",
        "                if target_encoder else pred_idx\n",
        "            )\n",
        "\n",
        "            lime_exp = lime_explainer.explain_instance(\n",
        "                input_df.values[0],\n",
        "                model.predict_proba,\n",
        "                num_features=10,\n",
        "                top_labels=1\n",
        "            )\n",
        "\n",
        "            st.success(f\"üéØ LIME explanation for class: **{label_name}**\")\n",
        "            st.pyplot(lime_exp.as_pyplot_figure(label=pred_idx))\n",
        "\n",
        "        else:\n",
        "            lime_exp = lime_explainer.explain_instance(\n",
        "                input_df.values[0],\n",
        "                model.predict,\n",
        "                num_features=10\n",
        "            )\n",
        "\n",
        "            st.success(\"üìà LIME explanation (Regression)\")\n",
        "            st.pyplot(lime_exp.as_pyplot_figure())\n",
        "\n",
        "        explained = True\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ======================================================\n",
        "# ‚ùå FINAL FALLBACK\n",
        "# ======================================================\n",
        "if not explained:\n",
        "    st.error(\n",
        "        \"‚ùå This model cannot be explained using SHAP or LIME.\\n\\n\"\n",
        "        \"Reason:\\n\"\n",
        "        \"- The selected model does not support probability outputs\\n\"\n",
        "        \"- SHAP TreeExplainer is not compatible\\n\\n\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PfYrivbXA0nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded965ca-0a4a-4710-e89b-87e7e9011d39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/5_Shap.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok config add-authtoken 35K0ERpK7SgOnSLLKN1IF9ov355_jhsFikHcuWL9dPugML5i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVu45hmHRT-3",
        "outputId": "e13e6330-042a-479b-ab4c-885c91f4f54b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Kill any previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background on port 8051\n",
        "get_ipython().system_raw('streamlit run Home.py --server.port 8502 &')\n",
        "\n",
        "public_url = ngrok.connect(8502)\n",
        "print(\"Click the public URL to open your app üëá\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go1p9QFdRUXn",
        "outputId": "15ca297b-f469-4608-bbb8-fffb7ba5ddaf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click the public URL to open your app üëá\n",
            "NgrokTunnel: \"https://shella-unmilitary-laically.ngrok-free.dev\" -> \"http://localhost:8502\"\n"
          ]
        }
      ]
    }
  ]
}