{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ePCf8m5HS7s",
        "outputId": "34b3f4bb-8105-4391-a6c9-f306c93bd53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.6)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=fdf3cd5091f093034aa2c02ad53f335afbce8248e101330897a7556130024a09\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit lime pyngrok xgboost shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.metrics import roc_auc_score,r2_score\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Drop_Test_Predict\",page_icon='üëæ',layout='centered')\n",
        "st.title(\"AutoML Application\")\n",
        "st.write(\"\"\"\n",
        "üì§ **Step 1: Upload Your Dataset (CSV) in Drop Tab**\n",
        "\n",
        "Dataset Format Requirements:\n",
        "\n",
        "To ensure smooth training and accurate predictions, your CSV file must follow these rules:\n",
        "\n",
        "‚úî The last column must be the Target column\n",
        "\n",
        "This is the variable the model will learn to predict.\n",
        "\n",
        "Example: loan_status, species, price, fraud_flag, etc.\n",
        "\n",
        "‚úî The first column should be an ID column\n",
        "\n",
        "This must contain unique identifiers such as:\n",
        "\n",
        "- customer_id, applicant_no, serial_number, transaction_id\n",
        "\n",
        "‚úî All other columns are used as features\n",
        "\n",
        "ü§ñ  **Step 2: Automatic Model Training**\n",
        "\n",
        "- Once uploaded, the app will:\n",
        "\n",
        "- Detect if the problem is classification or regression\n",
        "\n",
        "- Automatically clean and encode your data\n",
        "\n",
        "Try multiple ML models\n",
        "\n",
        "Select the best-performing model using industry-standard metrics (AUC for classification, R¬≤ for regression)\n",
        "\n",
        "üìù **Step 3: Enter New Values & Predict**\n",
        "\n",
        "After training:\n",
        "\n",
        "- The app generates input boxes for each feature\n",
        "\n",
        "- Enter values for a new sample\n",
        "\n",
        "- Click Predict to see the model‚Äôs output instantly\n",
        "\n",
        "üéØ **Purpose of This App**\n",
        "\n",
        "This platform is designed to:\n",
        "\n",
        "- Enable non-technical users to build ML models\n",
        "\n",
        "- Automate training and model selection\n",
        "\n",
        "- Provide quick predictions without writing code\n",
        "\n",
        "- Support any dataset with a clear target column\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF2xruGCoa8w",
        "outputId": "671c19e3-8e03-4453-daea-e2f0204e1a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pages"
      ],
      "metadata": {
        "id": "2tdT1OzzPjlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/1_Upload.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import kagglehub\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "st.set_page_config(page_title=\"Drop_Test_Predict\",page_icon='üëæ',layout='centered')\n",
        "\n",
        "if  \"df\"  in st.session_state:\n",
        "  st.success(\"‚úîÔ∏è File already uploaded\")\n",
        "  st.write(f\"**Preview: {st.session_state['dataset']}**\")\n",
        "  st.dataframe(st.session_state['df'].head())\n",
        "else:\n",
        "  def dataframe(Ufiles):\n",
        "    try:\n",
        "      dframe = pd.read_csv(Ufiles)\n",
        "    except Exception:\n",
        "      try:\n",
        "        dframe = pd.read_excel(Ufiles)\n",
        "      except Exception as e:\n",
        "        st.error(\"‚ùå Could not read file. Unsupported format\")\n",
        "        st.stop()\n",
        "    return dframe\n",
        "\n",
        "  def Multi_dataframe(path,Ufiles):\n",
        "      all_df=[]\n",
        "      for file_name in Ufiles:\n",
        "          st.write(file_name)\n",
        "          if file_name.endswith((\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\")):\n",
        "            all_df.append(dataframe(os.path.join(path,file_name)))\n",
        "      df_common = set(all_df[0].columns)\n",
        "      for dfs in all_df[1:]:\n",
        "        df_common = df_common.intersection(dfs.columns)\n",
        "      df_common = list(df_common)\n",
        "      st.write(f\"**üé∞Common features: {df_common[0]}**\")\n",
        "      df_merged = reduce(lambda df1, df2: pd.merge(df1, df2, on=df_common, how='outer'), all_df)\n",
        "      return df_merged\n",
        "\n",
        "  st.title(\"üì• Upload Dataset or Enter URL\")\n",
        "  uploaded_file = st.file_uploader(\"Upload CSV/Excel file or ZIP  file\", type=[\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\"])\n",
        "  st.write(\"***\")\n",
        "  data_url = st.text_input(\"Enter URL :\",placeholder=\"Paste a direct CSV/Excel or Kaggle Hub link here\")\n",
        "  df=None\n",
        "\n",
        "  if uploaded_file is not None:\n",
        "    if uploaded_file.name.split('.')[-1] ==\"zip\":\n",
        "      path=\"Data_folder\"\n",
        "      st.write(\"Multiple datasets detected...\")\n",
        "      with zipfile.ZipFile(uploaded_file, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(path)\n",
        "        files = os.listdir(path)\n",
        "        with st.spinner(\"Merging all Dataset\"):\n",
        "          df=Multi_dataframe(path,files)\n",
        "        st.session_state['df']=df\n",
        "        st.session_state[\"dataset\"]=uploaded_file.name.split('.')[0]\n",
        "    else:\n",
        "      df=dataframe(uploaded_file)\n",
        "      st.success(\"File uploaded successfully!\")\n",
        "      st.session_state['df']=df\n",
        "      st.session_state[\"dataset\"]=uploaded_file.name.split('.')[0]\n",
        "  elif data_url:\n",
        "    if data_url.split('.')[-1] not in (\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\"):\n",
        "      st.write(\"**Kaggle hub dataset**\")\n",
        "      path = kagglehub.dataset_download(data_url)\n",
        "      files = os.listdir(path)\n",
        "      if len(files)>1:\n",
        "        st.write(\"**üìöMultiple datasets detected**\")\n",
        "        with st.spinner(\"**üìíMerging all Dataset**\"):\n",
        "          df=Multi_dataframe(path,files)\n",
        "      else:\n",
        "        df = dataframe(os.path.join(path, files[0]))\n",
        "    else:\n",
        "      df=dataframe(data_url)\n",
        "    st.session_state[\"dataset\"]=data_url.split('/')[-1]\n",
        "    st.session_state['df']=df\n",
        "  else:\n",
        "    pass\n",
        "  if df is not None:\n",
        "    st.write(f\"**Preview: {st.session_state['dataset']}**\")\n",
        "    st.dataframe(st.session_state['df'].head())\n"
      ],
      "metadata": {
        "id": "ACi49vV4jZQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b41138e-07f7-4fd2-dd48-58efc7390a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/1_Upload.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/2_preprocessing.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "st.set_page_config(page_title=\"Drop_Test_Predict\",page_icon='üëæ',layout='centered')\n",
        "st.title(\"ü§ñ Train Model\")\n",
        "if \"df\" not in st.session_state:\n",
        "    st.error(\"‚ùå No dataset uploaded. Go to Drop page first.\")\n",
        "    st.stop()\n",
        "\n",
        "df=st.session_state[\"df\"]\n",
        "st.dataframe(df.head())\n",
        "target_col = st.selectbox(\"üéØ Select Target Column (Prediction Output Column)\",options=df.columns)\n",
        "\n",
        "drop_cols = st.multiselect(\" Select Unwanted Columns to Drop (e.g., ID, Serial No, Unique Key)\",options=[col for col in df.columns if col != target_col])\n",
        "\n",
        "if st.button(\"Apply Selection\"):\n",
        "    df.drop(columns=drop_cols,inplace=True)\n",
        "    target = df[target_col]\n",
        "    if target.dtype in ['int64', 'float64']:\n",
        "        unique_vals = target.nunique()\n",
        "        if unique_vals <= 20:\n",
        "            p_type = \"classification\"\n",
        "        else:\n",
        "            p_type = \"regression\"\n",
        "    else:\n",
        "        p_type = \"classification\"\n",
        "\n",
        "    st.write(f\"Dataset: **{st.session_state['dataset']}**\")\n",
        "    st.session_state['p_type']=p_type\n",
        "    st.write(f\"Detected problem type : **{p_type}**\")\n",
        "    #Preprocessing\n",
        "    #Handling duplicates\n",
        "    with st.spinner(\"üöÄ Optimizing your dataset for best model performance...\"):\n",
        "      if df.duplicated().any():\n",
        "        df=df.drop_duplicates()\n",
        "      df = df.dropna(subset=[df.columns[-1]])\n",
        "      X = df.drop(columns=target_col)\n",
        "      Y = df[target_col]\n",
        "      for_col=[]\n",
        "      for_uniq=[]\n",
        "      for col in  X.select_dtypes(include=[\"object\"]).columns:\n",
        "        for_uniq.append(X[col].unique().tolist())\n",
        "        for_col.append(col)\n",
        "      all=dict(zip(for_col,for_uniq))\n",
        "      st.session_state[\"cat_options\"]=all\n",
        "      st.session_state['features']=X\n",
        "      #Handling missing values\n",
        "      missing=X.isna().mean().to_dict()\n",
        "      num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "      #cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "      X_encoders = {}\n",
        "      for f,m in missing.items():\n",
        "        if m > 0.5:\n",
        "          X.drop(columns=f,inplace=True)\n",
        "        else:\n",
        "          if f in num_cols:\n",
        "            X[f].fillna(X[f].median(),inplace=True)\n",
        "            Q1 = X[f].quantile(0.25)\n",
        "            Q3 = X[f].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            no_of_outliers=((X[f]<lower_bound)|(X[f]>upper_bound)).sum()\n",
        "            if (no_of_outliers/len(X[f]))>0.1:\n",
        "              X[f] = X[f].clip(lower=lower_bound, upper=upper_bound)\n",
        "          else:\n",
        "            X[f].fillna(X[f].mode()[0],inplace=True)\n",
        "            le = LabelEncoder()\n",
        "            X[f] = le.fit_transform(X[f].astype(str))\n",
        "            X_encoders[f] = le\n",
        "      st.session_state['encoders']=X_encoders\n",
        "      #handling outliers\n",
        "      if p_type=='regression':\n",
        "        Q1 = Y.quantile(0.25)\n",
        "        Q3 = Y.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        Y = Y.clip(lower=lower_bound, upper=upper_bound)\n",
        "      else:\n",
        "        target_encoder = LabelEncoder()\n",
        "        Y = pd.DataFrame(target_encoder.fit_transform(Y.astype(str)), columns=[target_col])\n",
        "        st.session_state['target_encoder']=target_encoder\n",
        "      #encoding\n",
        "      time.sleep(3)\n",
        "      st.session_state['X']=X\n",
        "      st.session_state['Y']=Y\n",
        "      st.success(\"‚ú® Preprocessing Complete! Data is now ready for model training üöÄ\")\n",
        "      st.write(\"üßÆFeatures Previews: \")\n",
        "      st.dataframe(X.head())\n",
        "      st.write(\"üîçPrediction Preview: \")\n",
        "      st.dataframe(Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2yWeBPtsM_J",
        "outputId": "4c74097c-5550-436e-b804-5c398305255a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/2_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/3_train.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split, cross_val_score,RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder,label_binarize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "# Train-test split\n",
        "X=st.session_state['X']\n",
        "Y=st.session_state['Y']\n",
        "p_type=st.session_state['p_type']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
        "\n",
        "models = {}\n",
        "\n",
        "if p_type == \"classification\":\n",
        "    models = {\n",
        "        \"Logistic Regression\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", LogisticRegression(max_iter=500))]),\n",
        "        \"SVM\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", SVC(probability=True))]),\n",
        "        \"KNN\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", KNeighborsClassifier())]),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "    }\n",
        "else:\n",
        "    models = {\n",
        "        \"Linear Regression\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", LinearRegression())]),\n",
        "        \"SVR\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", SVR())]),\n",
        "        \"KNN Regressor\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", KNeighborsRegressor())]),\n",
        "        \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
        "        \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
        "        \"XGBoost Regressor\": XGBRegressor(random_state=42)\n",
        "    }\n",
        "\n",
        "results = {}\n",
        "best_model = None\n",
        "best_score = 0\n",
        "\n",
        "with st.spinner(\" Training multiple models & selecting the best one...\"):\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        classes = sorted(np.unique(y_test))\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "          if len(classes) == 2:  # Binary classification\n",
        "              score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "          else:  # Multi-class classification\n",
        "              score = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
        "        else:\n",
        "            score = r2_score(y_test, y_pred)\n",
        "\n",
        "        results[name] = score\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "results_df = pd.DataFrame({\"Model\": results.keys(), \"Score\": results.values()},)\n",
        "results_df.index=results_df.index+1\n",
        "results_df = results_df.sort_values(by=\"Score\", ascending=False)\n",
        "\n",
        "st.success(\"üéØ Model Training Completed!\")\n",
        "st.write(\"### ü•áü•àü•â Top 3 models\")\n",
        "st.dataframe(results_df.head(3))\n",
        "#Hyperparameter tuning for Classification\n",
        "param_grid_classification = {\n",
        "    \"Logistic Regression\": {\n",
        "        \"model__C\": [0.01, 0.1, 1, 10],\n",
        "        \"model__penalty\": [\"l2\"]\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        \"model__C\": [0.1, 1, 10],\n",
        "        \"model__kernel\": [\"linear\", \"rbf\"],\n",
        "        \"model__gamma\": [\"scale\", \"auto\"]\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        \"model__n_neighbors\": [3, 5, 7, 11],\n",
        "        \"model__weights\": [\"uniform\", \"distance\"]\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"n_estimators\": [100, 200, 400],\n",
        "        \"max_depth\": [None, 10, 20, 30],\n",
        "        \"min_samples_split\": [2, 5, 10],\n",
        "        \"min_samples_leaf\": [1, 2, 4],\n",
        "    },\n",
        "    \"Decision Tree\": {\n",
        "        \"max_depth\": [None, 10, 20, 30],\n",
        "        \"criterion\": [\"gini\", \"entropy\"]\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"n_estimators\": [200, 400, 600,800],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "        \"max_depth\": [3, 5, 7,9],\n",
        "        \"subsample\": [0.7, 0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "#Hyper parameter tuning for regression\n",
        "param_grid_regression = {\n",
        "    \"Linear Regression\": {},  # No tuning needed\n",
        "    \"SVR\": {\n",
        "        \"model__C\": [0.1, 1, 10],\n",
        "        \"model__kernel\": [\"rbf\", \"linear\"],\n",
        "        \"model__gamma\": [\"scale\", \"auto\"]\n",
        "    },\n",
        "    \"KNN Regressor\": {\n",
        "        \"model__n_neighbors\": [3, 5, 7, 11],\n",
        "        \"model__weights\": [\"uniform\", \"distance\"]\n",
        "    },\n",
        "    \"Random Forest Regressor\": {\n",
        "        \"n_estimators\": [100, 400, 600,800],\n",
        "        \"max_depth\": [None, 10, 20, 30,50],\n",
        "        \"min_samples_split\": [2, 5, 7,10]\n",
        "    },\n",
        "    \"Decision Tree Regressor\": {\n",
        "        \"max_depth\": [None, 10, 20, 30],\n",
        "        \"criterion\": [\"squared_error\", \"absolute_error\"]\n",
        "    },\n",
        "    \"XGBoost Regressor\": {\n",
        "        \"n_estimators\": [200, 400, 600,800],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "        \"max_depth\": [3,5,7,9],\n",
        "        \"subsample\": [0.7, 0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Pick top 3 models from baseline,to perform hyperparameter tuning\n",
        "top2 = results_df.head(3)[\"Model\"].tolist()\n",
        "\n",
        "if p_type == \"classification\":\n",
        "    param_grid = param_grid_classification\n",
        "    scoring_metric = \"roc_auc\"\n",
        "else:\n",
        "    param_grid = param_grid_regression\n",
        "    scoring_metric = \"r2\"\n",
        "best_tuned_model_name = None\n",
        "best_tuned_model = None\n",
        "best_tuned_score = 0\n",
        "\n",
        "st.write(\"### üîß Hyperparameter tuning in progress for top 3 models...\")\n",
        "\n",
        "for model_name in top2:\n",
        "    st.write(f\"‚è≥ Tuning: **{model_name}** ...\")\n",
        "    model = clone(models[model_name])\n",
        "\n",
        "    if param_grid[model_name] == {}:   # No tuning required\n",
        "      tuned_model = clone(model)      # make a fresh copy\n",
        "      tuned_model.fit(X_train, y_train)  # train the model\n",
        "\n",
        "    else:\n",
        "        rs = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=param_grid[model_name],\n",
        "            n_iter=10,\n",
        "            scoring=scoring_metric,\n",
        "            cv=5,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        rs.fit(X_train, y_train)\n",
        "        tuned_model = rs.best_estimator_\n",
        "\n",
        "    y_pred = tuned_model.predict(X_test)\n",
        "    if p_type == \"classification\":\n",
        "      classes = sorted(np.unique(y_test))\n",
        "      if len(classes) == 2:\n",
        "        score = roc_auc_score(y_test,tuned_model.predict_proba(X_test)[:, 1])\n",
        "      else:\n",
        "        score = roc_auc_score(y_test, tuned_model.predict_proba(X_test), multi_class='ovr')\n",
        "    else:\n",
        "      score = r2_score(y_test, y_pred)\n",
        "\n",
        "    if score > best_tuned_score:\n",
        "        best_tuned_score = score\n",
        "        best_tuned_model = tuned_model\n",
        "        best_tuned_model_name = model_name\n",
        "\n",
        "st.success(\"üèÜ Hyperparameter tuning complete!\")\n",
        "st.write(f\"### Best Tuned Model: **{best_tuned_model_name}**\")\n",
        "st.write(f\"### Best Tuned Score: **{best_tuned_score:.4f}**\")\n",
        "st.session_state[\"best_model\"] = best_tuned_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apq7t82IJQRF",
        "outputId": "025bff24-7e7a-49e6-ee4a-4d74696b5d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/3_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/4_predict.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "st.set_page_config(page_title=\"Predict\", page_icon=\"üéØ\", layout=\"centered\")\n",
        "st.title(\"üîÆ Make Prediction\")\n",
        "\n",
        "# Check if model exists\n",
        "if \"best_model\" not in st.session_state or \"X\" not in st.session_state:\n",
        "    st.error(\"‚ùå Model not trained yet. Please go to Train page first.\")\n",
        "    st.stop()\n",
        "\n",
        "model = st.session_state[\"best_model\"]\n",
        "X_train_cols = st.session_state[\"X\"].columns   # processed columns\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "encoders = st.session_state.get(\"encoders\", {})\n",
        "target_encoder = st.session_state.get(\"target_encoder\", None)\n",
        "cat_values = st.session_state.get(\"cat_options\", {})\n",
        "\n",
        "st.write(\"### üìù Enter values for prediction:\")\n",
        "\n",
        "# Create input controls dynamically\n",
        "input_values = {}\n",
        "for col in X_train_cols:\n",
        "    if col in encoders:  # categorical\n",
        "        options = cat_values[col] if col in cat_values else list(encoders[col].classes_)\n",
        "        input_values[col] = st.selectbox(f\"{col}\", options=options)\n",
        "    else:\n",
        "        input_values[col] = st.number_input(f\"{col}\", value=float(st.session_state[\"X\"][col].median()))\n",
        "\n",
        "# Convert input to dataframe\n",
        "input_df = pd.DataFrame([input_values])\n",
        "st.session_state['input_df']=input_df\n",
        "# Encode categorical values\n",
        "for col in input_df.columns:\n",
        "    if col in encoders:\n",
        "        try:\n",
        "            input_df[col] = encoders[col].transform(input_df[col].astype(str))\n",
        "        except:\n",
        "            st.warning(f\"‚ö† Unknown category in '{col}'. Using default value.\")\n",
        "            input_df[col] = encoders[col].transform([encoders[col].classes_[0]])\n",
        "\n",
        "# Prediction\n",
        "if st.button(\"Predict\"):\n",
        "  try:\n",
        "    prediction = model.predict(input_df)[0]\n",
        "    st.session_state['prediction'] = prediction  # store prediction\n",
        "    st.session_state['last_input'] = input_df\n",
        "    if p_type == \"classification\":\n",
        "        proba = model.predict_proba(input_df)[0]\n",
        "        st.session_state['proba']=proba\n",
        "\n",
        "        if target_encoder:\n",
        "            prediction_label = target_encoder.inverse_transform([int(prediction)])[0]\n",
        "        else:\n",
        "            prediction_label = prediction\n",
        "\n",
        "        st.success(f\"üéØ Predicted Class: **{prediction_label}**\")\n",
        "    else:  # Regression\n",
        "        st.success(f\"üìå Predicted Value: **{prediction:.4f}**\")\n",
        "\n",
        "  except Exception as e:\n",
        "        st.error(f\"‚ùå Prediction failed: {e}\")\n",
        "# ---- PRINT BUTTON ----\n",
        "if st.button(\"Print / Save Result\"):\n",
        "    if \"prediction\" not in st.session_state:\n",
        "        st.error(\"‚ö† Please make a prediction first.\")\n",
        "    else:\n",
        "        save_row = st.session_state['last_input'].copy()\n",
        "        save_row[\"prediction\"] = st.session_state['prediction']\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            save_row[\"probability\"] = float(st.session_state[\"proba\"].max())\n",
        "\n",
        "        save_file = \"prediction_history.csv\"\n",
        "\n",
        "        # Write/append to file\n",
        "        if not os.path.exists(save_file):\n",
        "            save_row.to_csv(save_file, mode='w', header=True, index=False)\n",
        "        else:\n",
        "            save_row.to_csv(save_file, mode='a', header=False, index=False)\n",
        "\n",
        "        st.success(\"üìÅ Prediction saved successfully!\")\n",
        "\n",
        "        # Add download button\n",
        "        df_saved = pd.read_csv(save_file)\n",
        "        st.download_button(\n",
        "            label=\"üì• Download Prediction History\",\n",
        "            data=df_saved.to_csv(index=False),\n",
        "            file_name=\"prediction_history.csv\",\n",
        "            mime=\"text/csv\"\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq3E_Kyq4sh",
        "outputId": "d27257cc-0553-4296-d1a5-8013b5cbb546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/4_predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/5_Shap.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "st.write(\"## üîç Model Explainability\")\n",
        "model = st.session_state[\"best_model\"]\n",
        "input_df = st.session_state[\"input_df\"]\n",
        "X_train = st.session_state[\"X\"]\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "prediction=st.session_state['prediction']\n",
        "try:\n",
        "    # Attempt SHAP TreeExplainer\n",
        "    st.info(\"ü™µ Trying SHAP TreeExplainer...\")\n",
        "    shap.initjs()\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(input_df)\n",
        "\n",
        "    # Classification / regression handling\n",
        "    if p_type == \"classification\":\n",
        "        pred_class = int(prediction)\n",
        "        if isinstance(shap_values, list):  # multi-class\n",
        "            shap_vals = shap_values[pred_class][0]\n",
        "            base_val = explainer.expected_value[pred_class]\n",
        "        else:  # binary\n",
        "            shap_vals = shap_values[0]\n",
        "            base_val = explainer.expected_value\n",
        "    else:\n",
        "        shap_vals = shap_values[0]\n",
        "        base_val = explainer.expected_value\n",
        "\n",
        "    explanation = shap.Explanation(\n",
        "        values=shap_vals,\n",
        "        base_values=base_val,\n",
        "        data=input_df.iloc[0].values,\n",
        "        feature_names=input_df.columns.tolist()\n",
        "    )\n",
        "\n",
        "    st.success(\"‚úî SHAP explanation generated successfully!\")\n",
        "\n",
        "    # Waterfall plot\n",
        "    st.write(\"### ‚¨á SHAP Waterfall Plot\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "    shap.plots.waterfall(explanation, max_display=10, show=False)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Summary bar plot\n",
        "    st.write(\"### üìä SHAP Feature Importance Ranking\")\n",
        "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
        "    shap.plots.bar(explanation, show=False)\n",
        "    st.pyplot(fig2)\n",
        "\n",
        "except Exception as e:\n",
        "    # If SHAP fails ‚Üí fallback to LIME\n",
        "    st.warning(f\"‚ö† SHAP not available for this model. Switching to LIME automatically.\\nError: {e}\")\n",
        "\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Build lime explainer\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    X_train.values,\n",
        "    feature_names=X_train.columns.tolist(),\n",
        "    class_names=np.unique(st.session_state[\"Y\"]),\n",
        "    discretize_continuous=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "if p_type == \"classification\":\n",
        "    pred_class_idx = int(prediction)         # numeric class index\n",
        "    class_names = np.unique(st.session_state[\"Y\"])\n",
        "    target_encoder=st.session_state['target_encoder']\n",
        "    # If encoding exists, reverse translate to actual label\n",
        "    if target_encoder:\n",
        "        pred_class_label = target_encoder.inverse_transform([pred_class_idx])[0]\n",
        "    else:\n",
        "        pred_class_label = class_names[pred_class_idx]\n",
        "\n",
        "    # Generate LIME explanation for predicted class\n",
        "    lime_exp = lime_explainer.explain_instance(\n",
        "        input_df.values[0],\n",
        "        model.predict_proba,\n",
        "        num_features=10,\n",
        "        top_labels=1\n",
        "    )\n",
        "\n",
        "    st.success(f\"üéØ LIME explanation for class: **{pred_class_label}**\")\n",
        "\n",
        "    fig3 = lime_exp.as_pyplot_figure(label=pred_class_idx)\n",
        "    st.pyplot(fig3)\n",
        "\n",
        "else:  # Regression\n",
        "    lime_explainer = LimeTabularExplainer(\n",
        "        X_train.values,\n",
        "        feature_names=X_train.columns.tolist(),\n",
        "        discretize_continuous=True,\n",
        "        verbose=True\n",
        "    )\n",
        "    lime_exp = lime_explainer.explain_instance(\n",
        "        input_df.values[0],\n",
        "        model.predict,\n",
        "        num_features=10\n",
        "    )\n",
        "\n",
        "    st.write(\"### üí° LIME Explanation (Regression)\")\n",
        "    fig3 = lime_exp.as_pyplot_figure()\n",
        "    st.pyplot(fig3)\n"
      ],
      "metadata": {
        "id": "PfYrivbXA0nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0447a78e-d81d-41ae-d03e-5b83270f702e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/5_Shap.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok config add-authtoken 35K0ERpK7SgOnSLLKN1IF9ov355_jhsFikHcuWL9dPugML5i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVu45hmHRT-3",
        "outputId": "57ba0e53-9dc4-4279-89e1-c1cbab05e4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Kill any previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background on port 8051\n",
        "get_ipython().system_raw('streamlit run Home.py --server.port 8502 &')\n",
        "\n",
        "public_url = ngrok.connect(8502)\n",
        "print(\"Click the public URL to open your app üëá\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go1p9QFdRUXn",
        "outputId": "5e77b630-f311-4dc9-82c4-9c9b53b0fec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click the public URL to open your app üëá\n",
            "NgrokTunnel: \"https://shella-unmilitary-laically.ngrok-free.dev\" -> \"http://localhost:8502\"\n"
          ]
        }
      ]
    }
  ]
}