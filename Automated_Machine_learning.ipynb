{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ePCf8m5HS7s"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit lime pyngrok xgboost shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AutoML Application\",\n",
        "    page_icon=\"ğŸ¤–\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "st.title(\"ğŸ¤– AutoML Application\")\n",
        "st.subheader(\"Build, Train, Predict & Explain ML Models Automatically\")\n",
        "\n",
        "st.write(\"\"\"\n",
        "---\n",
        "\n",
        "### ğŸ“¤ **Step 1: Upload Dataset**\n",
        "Go to the **Drop** tab and upload your dataset (CSV / Excel).\n",
        "\n",
        "The application will:\n",
        "- Preview your dataset\n",
        "- Let you select the target (prediction) column\n",
        "- Allow dropping unnecessary columns (ID, serial number, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§¹ **Step 2: Automatic Preprocessing**\n",
        "Once you confirm your selections, the app will automatically:\n",
        "- Detect **classification or regression**\n",
        "- Handle missing values\n",
        "- Encode categorical features\n",
        "- Remove duplicates and extreme outliers\n",
        "- Detect **class imbalance**\n",
        "- Apply **SMOTE automatically (only for training data)** when required\n",
        "\n",
        "> âš ï¸ SMOTE is applied safely to avoid data leakage.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ‹ï¸ **Step 3: Intelligent Model Training**\n",
        "The system trains **multiple ML models automatically**, including:\n",
        "- Logistic / Linear Regression\n",
        "- Support Vector Machines (SVM)\n",
        "- KNN\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost\n",
        "\n",
        "Each model is evaluated using:\n",
        "- **ROC-AUC** (Classification)\n",
        "- **RÂ² Score** (Regression)\n",
        "\n",
        "âœ… The best baseline model is selected\n",
        "âœ… Hyperparameter tuning is applied **only if it improves performance**\n",
        "âœ… The final best model is stored automatically\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ **Step 4: Prediction**\n",
        "After training:\n",
        "- Input fields are generated dynamically from your dataset\n",
        "- Supports both numerical & categorical features\n",
        "- One-click prediction\n",
        "- Shows:\n",
        "  - Predicted class / value\n",
        "  - Prediction probability (for classification)\n",
        "\n",
        "You can also:\n",
        "- Save predictions\n",
        "- Download prediction history as CSV\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” **Step 5: Model Explainability (SHAP / LIME)**\n",
        "Understand **why** the model made a prediction:\n",
        "- SHAP explanations for supported models\n",
        "- Automatic fallback when SHAP/LIME is not supported\n",
        "- Feature impact visualization\n",
        "- Safe handling for models without probability output\n",
        "\n",
        "> If a model cannot be explained, the app clearly informs the user.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ **Purpose of This Application**\n",
        "This AutoML platform is designed to:\n",
        "- Enable **non-technical users** to build ML models\n",
        "- Automate the **entire ML pipeline**\n",
        "- Prevent common ML mistakes (data leakage, imbalance bias)\n",
        "- Provide **transparent & explainable predictions**\n",
        "- Work with **any structured dataset**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ How to Start?\n",
        "â¡ï¸ Upload a dataset from the **Drop** page to begin.\n",
        "\"\"\")\n",
        "\n",
        "st.success(\"âœ” Ready to build intelligent ML models without writing code!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF2xruGCoa8w",
        "outputId": "3d6fe885-b0c1-4733-ffa3-0bec1dcbc2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pages"
      ],
      "metadata": {
        "id": "2tdT1OzzPjlV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/1_Upload.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import kagglehub\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "st.set_page_config(page_title=\"AutoML\",page_icon='ğŸ‘¾',layout='centered')\n",
        "\n",
        "if  \"df\"  in st.session_state:\n",
        "  st.title(\"ğŸ“¥ Dataset upload\")\n",
        "  st.success(\"âœ”ï¸ File already uploaded\")\n",
        "  st.write(f\"**Preview: {st.session_state['dataset']}**\")\n",
        "  st.dataframe(st.session_state['df'].head())\n",
        "else:\n",
        "  def dataframe(Ufiles):\n",
        "    try:\n",
        "      dframe = pd.read_csv(Ufiles)\n",
        "    except Exception:\n",
        "      try:\n",
        "        dframe = pd.read_excel(Ufiles)\n",
        "      except Exception as e:\n",
        "        st.error(\"âŒ Could not read file. Unsupported format\")\n",
        "        st.stop()\n",
        "    return dframe\n",
        "\n",
        "  def Multi_dataframe(path,Ufiles):\n",
        "      all_df=[]\n",
        "      for file_name in Ufiles:\n",
        "          st.write(file_name)\n",
        "          if file_name.endswith((\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\")):\n",
        "            all_df.append(dataframe(os.path.join(path,file_name)))\n",
        "      df_common = set(all_df[0].columns)\n",
        "      for dfs in all_df[1:]:\n",
        "        df_common = df_common.intersection(dfs.columns)\n",
        "      df_common = list(df_common)\n",
        "      st.write(f\"**ğŸ°Common features: {df_common[0]}**\")\n",
        "      df_merged = reduce(lambda df1, df2: pd.merge(df1, df2, on=df_common, how='outer'), all_df)\n",
        "      return df_merged\n",
        "\n",
        "  st.title(\"ğŸ“¥ Upload Dataset or Enter URL\")\n",
        "  uploaded_file = st.file_uploader(\"Upload CSV/Excel file or ZIP  file\", type=[\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\",\"zip\"])\n",
        "  st.write(\"***\")\n",
        "  data_url = st.text_input(\"Enter URL :\",placeholder=\"Paste a direct CSV/Excel or Kaggle Hub link here\")\n",
        "  df=None\n",
        "\n",
        "  if uploaded_file is not None:\n",
        "    if uploaded_file.name.split('.')[-1] ==\"zip\":\n",
        "      path=\"Data_folder\"\n",
        "      st.write(\"Multiple datasets detected...\")\n",
        "      with zipfile.ZipFile(uploaded_file, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(path)\n",
        "        files = os.listdir(path)\n",
        "        with st.spinner(\"Merging all Dataset\"):\n",
        "          df=Multi_dataframe(path,files)\n",
        "        st.session_state['df']=df\n",
        "        st.session_state[\"dataset\"]=uploaded_file.name.split('.')[0]\n",
        "    else:\n",
        "      df=dataframe(uploaded_file)\n",
        "      st.success(\"File uploaded successfully!\")\n",
        "      st.session_state['df']=df\n",
        "      st.session_state[\"dataset\"]=uploaded_file.name.split('.')[0]\n",
        "  elif data_url:\n",
        "    if data_url.split('.')[-1] not in (\"csv\", \"xls\", \"xlsx\", \"xls\", \"xlsb\", \"xlsm\", \"ods\"):\n",
        "      st.write(\"**Kaggle hub dataset**\")\n",
        "      path = kagglehub.dataset_download(data_url)\n",
        "      files = os.listdir(path)\n",
        "      if len(files)>1:\n",
        "        st.write(\"**ğŸ“šMultiple datasets detected**\")\n",
        "        with st.spinner(\"**ğŸ“’Merging all Dataset**\"):\n",
        "          df=Multi_dataframe(path,files)\n",
        "      else:\n",
        "        df = dataframe(os.path.join(path, files[0]))\n",
        "    else:\n",
        "      df=dataframe(data_url)\n",
        "    st.session_state[\"dataset\"]=data_url.split('/')[-1]\n",
        "    st.session_state['df']=df\n",
        "  else:\n",
        "    pass\n",
        "  if df is not None:\n",
        "    st.write(f\"**Preview: {st.session_state['dataset']}**\")\n",
        "    st.dataframe(st.session_state['df'].head())\n"
      ],
      "metadata": {
        "id": "ACi49vV4jZQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d1b056-bfb3-4216-c63a-d43bea94502c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/1_Upload.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/2_preprocessing.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "\n",
        "st.set_page_config(page_title=\"Drop_Test_Predict\", page_icon='ğŸ‘¾', layout='centered')\n",
        "st.title(\"ğŸ¤– Train Model\")\n",
        "\n",
        "# ---------------- CHECK DATA ----------------\n",
        "if \"df\" not in st.session_state:\n",
        "    st.error(\"âŒ No dataset uploaded. Go to Drop page first.\")\n",
        "    st.stop()\n",
        "\n",
        "# If already preprocessed\n",
        "if \"X\" in st.session_state and \"Y\" in st.session_state:\n",
        "    st.success(\"âœ¨ Preprocessing Complete! Data is ready for training ğŸš€\")\n",
        "    st.write(\"ğŸ§® Features Preview\")\n",
        "    st.dataframe(st.session_state[\"X\"].head())\n",
        "    st.write(\"ğŸ¯ Target Preview\")\n",
        "    st.dataframe(st.session_state[\"Y\"].head())\n",
        "    st.stop()\n",
        "\n",
        "df = st.session_state[\"df\"]\n",
        "st.dataframe(df.head())\n",
        "\n",
        "# ---------------- USER INPUT ----------------\n",
        "target_col = st.selectbox(\n",
        "    \"ğŸ¯ Select Target Column (Prediction Output Column)\",\n",
        "    options=df.columns\n",
        ")\n",
        "\n",
        "drop_cols = st.multiselect(\n",
        "    \"ğŸ—‘ Select Columns to Drop (ID / Serial / Irrelevant)\",\n",
        "    options=[c for c in df.columns if c != target_col]\n",
        ")\n",
        "\n",
        "# ---------------- HELPER ----------------\n",
        "def check_imbalance(y, threshold=0.4):\n",
        "    counts = Counter(y.squeeze())\n",
        "    ratio = min(counts.values()) / max(counts.values())\n",
        "    return ratio < threshold, ratio, dict(counts)\n",
        "\n",
        "# ---------------- APPLY ----------------\n",
        "if st.button(\"Apply Selection\"):\n",
        "\n",
        "    df = df.drop(columns=drop_cols)\n",
        "\n",
        "    target = df[target_col]\n",
        "\n",
        "    # Detect problem type\n",
        "    if target.dtype in [\"int64\", \"float64\"]:\n",
        "        p_type = \"classification\" if target.nunique() <= 20 else \"regression\"\n",
        "    else:\n",
        "        p_type = \"classification\"\n",
        "\n",
        "    st.session_state[\"p_type\"] = p_type\n",
        "    st.write(f\"Detected problem type: **{p_type.upper()}**\")\n",
        "\n",
        "    with st.spinner(\"ğŸš€ Preprocessing dataset...\"):\n",
        "\n",
        "        # Remove duplicates & missing target\n",
        "        df = df.drop_duplicates()\n",
        "        df = df.dropna(subset=[target_col])\n",
        "\n",
        "        X = df.drop(columns=target_col)\n",
        "        Y = df[target_col]\n",
        "\n",
        "        # Store categorical options\n",
        "        cat_options = {}\n",
        "        for col in X.select_dtypes(include=\"object\"):\n",
        "            cat_options[col] = X[col].unique().tolist()\n",
        "\n",
        "        st.session_state[\"cat_options\"] = cat_options\n",
        "\n",
        "        # Handle missing values & encoding\n",
        "        encoders = {}\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().mean() > 0.5:\n",
        "                X.drop(columns=col, inplace=True)\n",
        "                continue\n",
        "\n",
        "            if X[col].dtype in [\"int64\", \"float64\"]:\n",
        "                X[col].fillna(X[col].median(), inplace=True)\n",
        "                Q1 = X[col].quantile(0.25)\n",
        "                Q3 = X[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                no_of_outliers=((X[col]<lower_bound)|(X[col]>upper_bound)).sum()\n",
        "                if (no_of_outliers/len(X[col]))>0.1:\n",
        "                  X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "            else:\n",
        "                X[col].fillna(X[col].mode()[0], inplace=True)\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                encoders[col] = le\n",
        "\n",
        "        st.session_state[\"encoders\"] = encoders\n",
        "\n",
        "        # Target handling\n",
        "        if p_type == \"classification\":\n",
        "            target_encoder = LabelEncoder()\n",
        "            Y = pd.DataFrame(\n",
        "                target_encoder.fit_transform(Y.astype(str)),\n",
        "                columns=[target_col]\n",
        "            )\n",
        "            st.session_state[\"target_encoder\"] = target_encoder\n",
        "\n",
        "            # ---- IMBALANCE CHECK ----\n",
        "            is_imbalanced, ratio, dist = check_imbalance(Y)\n",
        "            st.session_state[\"is_imbalanced\"] = is_imbalanced\n",
        "            st.session_state[\"imbalance_ratio\"] = ratio\n",
        "            st.session_state[\"class_distribution\"] = dist\n",
        "\n",
        "            if is_imbalanced:\n",
        "                st.warning(\n",
        "                    f\"âš  Dataset is imbalanced (ratio={ratio:.2f}). \"\n",
        "                    \"SMOTE will be applied during training.\"\n",
        "                )\n",
        "            else:\n",
        "                st.success(\"âœ” Dataset is balanced\")\n",
        "\n",
        "        else:\n",
        "            #Regression outlier handling\n",
        "            Q1, Q3 = Y.quantile(0.25), Y.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            Y = Y.clip(Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
        "\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        st.session_state[\"X\"] = X\n",
        "        st.session_state[\"Y\"] = Y\n",
        "\n",
        "        st.success(\"âœ¨ Preprocessing Complete!\")\n",
        "        st.dataframe(X.head())\n",
        "        st.dataframe(Y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2yWeBPtsM_J",
        "outputId": "7bdaf3d4-0948-4a8b-c957-eff68ba5d935"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/2_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/3_train.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ---------------- STREAMLIT ----------------\n",
        "st.set_page_config(page_title=\"Train\", page_icon=\"ğŸ‹ï¸\", layout=\"centered\")\n",
        "st.title(\"ğŸ”„ Model Training\")\n",
        "\n",
        "if \"X\" not in st.session_state or \"Y\" not in st.session_state:\n",
        "    st.error(\"âŒ Please preprocess the dataset first.\")\n",
        "    st.stop()\n",
        "\n",
        "if \"best_model\" in st.session_state:\n",
        "    st.success(\"ğŸ† Model already trained\")\n",
        "    st.write(f\"**{st.session_state['best_tuned_model_name']}**\")\n",
        "    st.write(f\"Score: {st.session_state['best_tuned_score']:.4f}\")\n",
        "    st.stop()\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "X = st.session_state[\"X\"]\n",
        "Y = st.session_state[\"Y\"]\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "\n",
        "# Stratify if classification\n",
        "stratify = Y if p_type == \"classification\" else None\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=stratify\n",
        ")\n",
        "\n",
        "# ---------------- SMOTE ----------------\n",
        "if p_type == \"classification\" and st.session_state.get(\"is_imbalanced\", False):\n",
        "    min_class = min(st.session_state[\"class_distribution\"].values())\n",
        "    if min_class >= 10:\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "        st.info(\"âš– SMOTE applied on training data\")\n",
        "    else:\n",
        "        st.warning(\"âš  Too few minority samples â€” SMOTE skipped\")\n",
        "\n",
        "# ---------------- MODELS ----------------\n",
        "if p_type == \"classification\":\n",
        "    models = {\n",
        "        \"Logistic Regression\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", LogisticRegression(max_iter=1000))\n",
        "        ]),\n",
        "        \"SVM\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", SVC(probability=True))\n",
        "        ]),\n",
        "        \"KNN\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", KNeighborsClassifier())\n",
        "        ]),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42, class_weight=\"balanced\"),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"),\n",
        "        \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "    }\n",
        "else:\n",
        "    models = {\n",
        "        \"Linear Regression\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", LinearRegression())\n",
        "        ]),\n",
        "        \"SVR\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", SVR())\n",
        "        ]),\n",
        "        \"KNN Regressor\": Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", KNeighborsRegressor())\n",
        "        ]),\n",
        "        \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
        "        \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
        "        \"XGBoost Regressor\": XGBRegressor(random_state=42)\n",
        "    }\n",
        "\n",
        "# ---------------- BASELINE ----------------\n",
        "results = {}\n",
        "\n",
        "with st.spinner(\"ğŸ” Training baseline models...\"):\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            if len(np.unique(y_test)) == 2:\n",
        "                score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "            else:\n",
        "                score = roc_auc_score(\n",
        "                    y_test, model.predict_proba(X_test), multi_class=\"ovr\"\n",
        "                )\n",
        "        else:\n",
        "            score = r2_score(y_test, y_pred)\n",
        "\n",
        "        results[name] = score\n",
        "\n",
        "results_df = (\n",
        "    pd.DataFrame(results.items(), columns=[\"Model\", \"Score\"])\n",
        "    .sort_values(\"Score\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "st.success(\"âœ… Baseline training complete\")\n",
        "st.dataframe(results_df.head(3))\n",
        "\n",
        "# ---------------- SELECT BEST ----------------\n",
        "best_model_name = results_df.iloc[0][\"Model\"]\n",
        "best_score = results_df.iloc[0][\"Score\"]\n",
        "\n",
        "best_model = clone(models[best_model_name])\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------- SAVE ----------------\n",
        "st.success(\"ğŸ† Final Best Model Selected\")\n",
        "st.write(f\"**{best_model_name}**\")\n",
        "st.write(f\"Score: {best_score:.4f}\")\n",
        "\n",
        "st.session_state[\"best_model\"] = best_model\n",
        "st.session_state[\"best_tuned_model_name\"] = best_model_name\n",
        "st.session_state[\"best_tuned_score\"] = best_score\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apq7t82IJQRF",
        "outputId": "22ebc032-d00a-4b7b-a806-3176cb29f36e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/3_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/4_predict.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------------- PAGE CONFIG ----------------\n",
        "st.set_page_config(page_title=\"Predict\", page_icon=\"ğŸ¯\", layout=\"centered\")\n",
        "st.title(\"ğŸ”® Make Prediction\")\n",
        "\n",
        "# ---------------- CHECK TRAINED MODEL ----------------\n",
        "if \"best_model\" not in st.session_state or \"X\" not in st.session_state:\n",
        "    st.error(\"âŒ Model not trained yet. Please go to Train page first.\")\n",
        "    st.stop()\n",
        "\n",
        "# ---------------- LOAD SESSION OBJECTS ----------------\n",
        "model = st.session_state[\"best_model\"]\n",
        "X = st.session_state[\"X\"]\n",
        "X_cols = X.columns\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "encoders = st.session_state.get(\"encoders\", {})\n",
        "target_encoder = st.session_state.get(\"target_encoder\", None)\n",
        "cat_values = st.session_state.get(\"cat_options\", {})\n",
        "\n",
        "# ---------------- INIT SESSION STATE ----------------\n",
        "st.session_state.setdefault(\"input_values\", {})\n",
        "st.session_state.setdefault(\"prediction\", None)\n",
        "st.session_state.setdefault(\"proba\", None)\n",
        "st.session_state.setdefault(\"input_df\", None)\n",
        "st.session_state.setdefault(\"prediction_history\", pd.DataFrame())\n",
        "\n",
        "# ---------------- INPUT FORM ----------------\n",
        "st.write(\"### ğŸ“ Enter values for prediction:\")\n",
        "\n",
        "input_values = {}\n",
        "\n",
        "for col in X_cols:\n",
        "\n",
        "    if col in encoders:  # categorical\n",
        "        options = cat_values.get(col, list(encoders[col].classes_))\n",
        "        default = st.session_state[\"input_values\"].get(col, options[0])\n",
        "\n",
        "        input_values[col] = st.selectbox(\n",
        "            col,\n",
        "            options=options,\n",
        "            index=options.index(default) if default in options else 0\n",
        "        )\n",
        "\n",
        "    else:  # numerical\n",
        "        default = st.session_state[\"input_values\"].get(col, float(X[col].median()))\n",
        "        input_values[col] = st.number_input(col, value=float(default))\n",
        "\n",
        "# ---------------- PREP INPUT DF ----------------\n",
        "raw_input_df = pd.DataFrame([input_values])\n",
        "st.session_state[\"input_df\"] = raw_input_df.copy()\n",
        "\n",
        "encoded_df = raw_input_df.copy()\n",
        "for col in encoded_df.columns:\n",
        "    if col in encoders:\n",
        "        encoded_df[col] = encoders[col].transform(encoded_df[col].astype(str))\n",
        "\n",
        "# ---------------- PREDICT ----------------\n",
        "if st.button(\"Predict\"):\n",
        "    try:\n",
        "        st.session_state[\"input_values\"] = input_values\n",
        "\n",
        "        pred = model.predict(encoded_df)[0]\n",
        "        st.session_state[\"prediction\"] = pred\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            proba = model.predict_proba(encoded_df)[0]\n",
        "            st.session_state[\"proba\"] = proba\n",
        "\n",
        "        st.success(\"âœ… Prediction successful\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"âŒ Prediction failed: {e}\")\n",
        "\n",
        "# ---------------- SHOW RESULT ----------------\n",
        "if st.session_state[\"prediction\"] is not None:\n",
        "\n",
        "    if p_type == \"classification\":\n",
        "        idx = int(st.session_state[\"prediction\"])\n",
        "        label = (\n",
        "            target_encoder.inverse_transform([idx])[0]\n",
        "            if target_encoder else idx\n",
        "        )\n",
        "        confidence = st.session_state[\"proba\"].max()\n",
        "\n",
        "        st.success(f\"ğŸ¯ Predicted Class: **{label}**\")\n",
        "        st.info(f\"ğŸ“Š Confidence: **{confidence:.2%}**\")\n",
        "\n",
        "    else:\n",
        "        st.success(f\"ğŸ“Œ Predicted Value: **{st.session_state['prediction']:.4f}**\")\n",
        "\n",
        "# ---------------- SAVE PREDICTION ----------------\n",
        "if st.button(\"ğŸ’¾ Save Prediction\"):\n",
        "    if st.session_state[\"prediction\"] is None:\n",
        "        st.error(\"âš  Make a prediction first.\")\n",
        "    else:\n",
        "        row = st.session_state[\"input_df\"].copy()\n",
        "        row[\"prediction\"] = st.session_state[\"prediction\"]\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            row[\"confidence\"] = float(st.session_state[\"proba\"].max())\n",
        "\n",
        "        st.session_state[\"prediction_history\"] = pd.concat(\n",
        "            [st.session_state[\"prediction_history\"], row],\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "        st.success(\"ğŸ“ Prediction saved successfully!\")\n",
        "\n",
        "# ---------------- SHOW SAVED DATAFRAME ----------------\n",
        "if not st.session_state[\"prediction_history\"].empty:\n",
        "    st.write(\"### ğŸ“Š Prediction History\")\n",
        "    st.dataframe(st.session_state[\"prediction_history\"])\n",
        "\n",
        "    # ---------------- DOWNLOAD BUTTON ----------------\n",
        "    st.download_button(\n",
        "        label=\"ğŸ“¥ Download Prediction History\",\n",
        "        data=st.session_state[\"prediction_history\"].to_csv(index=False),\n",
        "        file_name=\"prediction_history.csv\",\n",
        "        mime=\"text/csv\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq3E_Kyq4sh",
        "outputId": "fa016c4a-2a4f-40ce-ad78-5100c426c336"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/4_predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/5_Shap_Lime.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "st.set_page_config(page_title=\"Explainability\", page_icon=\"ğŸ”\", layout=\"centered\")\n",
        "st.title(\"âš–ï¸ Model Explainability (SHAP / LIME)\")\n",
        "\n",
        "# ------------------ VALIDATION ------------------\n",
        "if \"input_df\" not in st.session_state or \"best_model\" not in st.session_state:\n",
        "    st.error(\"âŒ Make a prediction first to view explainability.\")\n",
        "    st.stop()\n",
        "\n",
        "model = st.session_state[\"best_model\"]\n",
        "input_df = st.session_state[\"input_df\"]\n",
        "X_train = st.session_state[\"X\"]\n",
        "Y_train = st.session_state[\"Y\"]\n",
        "p_type = st.session_state[\"p_type\"]\n",
        "prediction = st.session_state[\"prediction\"]\n",
        "target_encoder = st.session_state.get(\"target_encoder\", None)\n",
        "\n",
        "explained = False   # track whether explanation succeeded\n",
        "\n",
        "# ======================================================\n",
        "# ğŸ”¹ TRY SHAP\n",
        "# ======================================================\n",
        "try:\n",
        "    st.info(\"ğŸªµ Trying SHAP explanation...\")\n",
        "\n",
        "    shap.initjs()\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(input_df)\n",
        "\n",
        "    if p_type == \"classification\":\n",
        "        pred_idx = int(prediction)\n",
        "\n",
        "        if isinstance(shap_values, list):  # multiclass\n",
        "            values = shap_values[pred_idx][0]\n",
        "            base = explainer.expected_value[pred_idx]\n",
        "        else:  # binary\n",
        "            values = shap_values[0]\n",
        "            base = explainer.expected_value\n",
        "    else:\n",
        "        values = shap_values[0]\n",
        "        base = explainer.expected_value\n",
        "\n",
        "    explanation = shap.Explanation(\n",
        "        values=values,\n",
        "        base_values=base,\n",
        "        data=input_df.iloc[0].values,\n",
        "        feature_names=input_df.columns.tolist()\n",
        "    )\n",
        "\n",
        "    st.success(\"âœ” SHAP explanation generated\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    shap.plots.waterfall(explanation, show=False)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    explained = True\n",
        "\n",
        "except Exception:\n",
        "    st.warning(\"âš  SHAP not supported for this model.\")\n",
        "\n",
        "# ======================================================\n",
        "# ğŸ”¹ TRY LIME (ONLY IF SHAP FAILED)\n",
        "# ======================================================\n",
        "if not explained:\n",
        "    try:\n",
        "        st.info(\"ğŸ‹ Trying LIME explanation...\")\n",
        "\n",
        "        if p_type == \"classification\" and not hasattr(model, \"predict_proba\"):\n",
        "            raise NotImplementedError(\"Model has no probability scores\")\n",
        "\n",
        "        lime_explainer = LimeTabularExplainer(\n",
        "            training_data=X_train.values,\n",
        "            feature_names=X_train.columns.tolist(),\n",
        "            class_names=np.unique(Y_train).astype(str),\n",
        "            discretize_continuous=True\n",
        "        )\n",
        "\n",
        "        if p_type == \"classification\":\n",
        "            pred_idx = int(prediction)\n",
        "\n",
        "            label_name = (\n",
        "                target_encoder.inverse_transform([pred_idx])[0]\n",
        "                if target_encoder else pred_idx\n",
        "            )\n",
        "\n",
        "            lime_exp = lime_explainer.explain_instance(\n",
        "                input_df.values[0],\n",
        "                model.predict_proba,\n",
        "                num_features=10,\n",
        "                top_labels=1\n",
        "            )\n",
        "\n",
        "            st.success(f\"ğŸ¯ LIME explanation for class: **{label_name}**\")\n",
        "            st.pyplot(lime_exp.as_pyplot_figure(label=pred_idx))\n",
        "\n",
        "        else:\n",
        "            lime_exp = lime_explainer.explain_instance(\n",
        "                input_df.values[0],\n",
        "                model.predict,\n",
        "                num_features=10\n",
        "            )\n",
        "\n",
        "            st.success(\"ğŸ“ˆ LIME explanation (Regression)\")\n",
        "            st.pyplot(lime_exp.as_pyplot_figure())\n",
        "\n",
        "        explained = True\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ======================================================\n",
        "# âŒ FINAL FALLBACK\n",
        "# ======================================================\n",
        "if not explained:\n",
        "    st.error(\n",
        "        \"âŒ This model cannot be explained using SHAP or LIME.\\n\\n\"\n",
        "        \"Reason:\\n\"\n",
        "        \"- The selected model does not support probability outputs\\n\"\n",
        "        \"- SHAP TreeExplainer is not compatible\\n\\n\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PfYrivbXA0nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7092d9af-c1e1-486d-94e1-3fccd1e2031d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/5_Shap_Lime.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok config add-authtoken 35K0ERpK7SgOnSLLKN1IF9ov355_jhsFikHcuWL9dPugML5i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVu45hmHRT-3",
        "outputId": "5febbf31-77be-47a2-a61b-7d2fa3790158"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Kill any previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background on port 8051\n",
        "get_ipython().system_raw('streamlit run Home.py --server.port 8502 &')\n",
        "\n",
        "public_url = ngrok.connect(8502)\n",
        "print(\"Click the public URL to open your app ğŸ‘‡\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go1p9QFdRUXn",
        "outputId": "f4747848-5458-4b13-e14a-3bf9b4ff4d07"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click the public URL to open your app ğŸ‘‡\n",
            "NgrokTunnel: \"https://shella-unmilitary-laically.ngrok-free.dev\" -> \"http://localhost:8502\"\n"
          ]
        }
      ]
    }
  ]
}